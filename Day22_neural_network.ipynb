{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network example\n",
    "Let's explore implementation of Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import Vector, dot\n",
    "\n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)\n",
    "\n",
    "and_weights = [2., 2]\n",
    "and_bias = -3.\n",
    "\n",
    "assert perceptron_output(and_weights, and_bias, [1, 1]) == 1\n",
    "assert perceptron_output(and_weights, and_bias, [0, 1]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [1, 0]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [0, 0]) == 0\n",
    "\n",
    "or_weights = [2., 2]\n",
    "or_bias = -1.\n",
    "\n",
    "assert perceptron_output(or_weights, or_bias, [1, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [1, 0]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 0]) == 0\n",
    "\n",
    "not_weights = [-2.]\n",
    "not_bias = 1.\n",
    "\n",
    "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
    "assert perceptron_output(not_weights, not_bias, [1]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Sigmoid function and feedforward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    # weights includes the bias term, inputs includes a 1\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # Add a constant.\n",
    "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "                  for neuron in layer]                    # for each neuron.\n",
    "        outputs.append(output)                            # Add to results.\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Build the XOR gate that we couldn’t build with a single perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and_gate = min\n",
    "# or_gate = max\n",
    "# xor_gate = lambda x, y: 0 if x == y else 1 \n",
    "\n",
    "xor_network = [# hidden layer\n",
    "               [[20., 20, -30],      # 'and' neuron\n",
    "                [20., 20, -10]],     # 'or'  neuron\n",
    "               # output layer\n",
    "               [[-60., 60, -30]]]    # '2nd input but not 1st input' neuron\n",
    "\n",
    "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
    "# final output, and the [0] gets the value out of the resulting vector\n",
    "assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001\n",
    "assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000\n",
    "assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000\n",
    "assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://learning.oreilly.com/library/view/data-science-from/9781492041122/assets/dsf2_1803.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://learning.oreilly.com/library/view/data-science-from/9781492041122/assets/dsf2_1803.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "we use data to train neural networks. The typical approach is an algorithm called backpropagation, which uses gradient descent or one of its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]],\n",
    "                      input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll start by generating the training data and initializing our neural network with random weights\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# training data\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "# start with random weights\n",
    "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "             [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "            # output layer: 2 inputs -> 1 output\n",
    "            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor: 100%|█████████████████████████████████████████████████████| 20000/20000 [00:01<00:00, 11773.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# As usual, we can train it using gradient descent\n",
    "from gradient_descent import gradient_step\n",
    "import tqdm\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "    for x, y in zip(xs, ys):\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "        # Take a gradient step for each neuron in each layer\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                    for neuron, grad in zip(layer, layer_grad)]\n",
    "                   for layer, layer_grad in zip(network, gradients)]\n",
    "\n",
    "# check that it learned XOR\n",
    "assert feed_forward(network, [0, 0])[-1][0] < 0.01\n",
    "assert feed_forward(network, [0, 1])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 0])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 1])[-1][0] < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Fizz Buzz\n",
    "Print the numbers 1 to 100, except that if the number is divisible\n",
    "by 3, print \"fizz\"; if the number is divisible by 5, print \"buzz\";\n",
    "and if the number is divisible by 15, print \"fizzbuzz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(x: int) -> Vector:\n",
    "    if x % 15 == 0:\n",
    "        return [0, 0, 0, 1]\n",
    "    elif x % 5 == 0:\n",
    "        return [0, 0, 1, 0]\n",
    "    elif x % 3 == 0:\n",
    "        return [0, 1, 0, 0]\n",
    "    else:\n",
    "        return [1, 0, 0, 0]\n",
    "\n",
    "assert fizz_buzz_encode(2) == [1, 0, 0, 0]\n",
    "assert fizz_buzz_encode(6) == [0, 1, 0, 0]\n",
    "assert fizz_buzz_encode(10) == [0, 0, 1, 0]\n",
    "assert fizz_buzz_encode(30) == [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(x: int) -> Vector:\n",
    "    binary: List[float] = []\n",
    "\n",
    "    for i in range(10):\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "\n",
    "    return binary\n",
    "\n",
    "#                             1  2  4  8 16 32 64 128 256 512\n",
    "assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n",
    "assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s create a neural network with random initial weights\n",
    "NUM_HIDDEN = 25\n",
    "\n",
    "network = [\n",
    "    # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n",
    "    [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "\n",
    "    # output_layer: NUM_HIDDEN inputs -> 4 outputs\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fizz buzz (loss: 29.44): 100%|███████████████████████████████████████████████████████| 500/500 [01:53<00:00,  4.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from linear_algebra import squared_distance\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "with tqdm.trange(500) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = feed_forward(network, x)[-1]\n",
    "            epoch_loss += squared_distance(predicted, y)\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                        for neuron, grad in zip(layer, layer_grad)]\n",
    "                    for layer, layer_grad in zip(network, gradients)]\n",
    "\n",
    "        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our network will produce a four-dimensional vector of numbers, but we want a single prediction. \n",
    "#We’ll do that by taking the argmax, which is the index of the largest value:\n",
    "def argmax(xs: list) -> int:\n",
    "    \"\"\"Returns the index of the largest value\"\"\"\n",
    "    return max(range(len(xs)), key=lambda i: xs[i])\n",
    "\n",
    "assert argmax([0, -1]) == 0               # items[0] is largest\n",
    "assert argmax([-1, 0]) == 1               # items[1] is largest\n",
    "assert argmax([-1, 10, 5, 20, -3]) == 3   # items[3] is largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "2 2 2\n",
      "3 fizz fizz\n",
      "4 4 4\n",
      "5 buzz buzz\n",
      "6 fizz fizz\n",
      "7 7 7\n",
      "8 8 8\n",
      "9 fizz fizz\n",
      "10 buzz buzz\n",
      "11 11 11\n",
      "12 fizz fizz\n",
      "13 13 13\n",
      "14 14 14\n",
      "15 fizzbuzz fizzbuzz\n",
      "16 16 16\n",
      "17 17 17\n",
      "18 fizz fizz\n",
      "19 19 19\n",
      "20 20 buzz\n",
      "21 fizz fizz\n",
      "22 22 22\n",
      "23 23 23\n",
      "24 fizz fizz\n",
      "25 buzz buzz\n",
      "26 26 26\n",
      "27 fizz fizz\n",
      "28 28 28\n",
      "29 29 29\n",
      "30 fizzbuzz fizzbuzz\n",
      "31 31 31\n",
      "32 32 32\n",
      "33 fizz fizz\n",
      "34 34 34\n",
      "35 buzz buzz\n",
      "36 fizz fizz\n",
      "37 37 37\n",
      "38 38 38\n",
      "39 fizz fizz\n",
      "40 buzz buzz\n",
      "41 41 41\n",
      "42 fizz fizz\n",
      "43 43 43\n",
      "44 44 44\n",
      "45 fizzbuzz fizzbuzz\n",
      "46 46 46\n",
      "47 47 47\n",
      "48 fizz fizz\n",
      "49 49 49\n",
      "50 buzz buzz\n",
      "51 fizz fizz\n",
      "52 52 52\n",
      "53 53 53\n",
      "54 fizz fizz\n",
      "55 buzz buzz\n",
      "56 56 56\n",
      "57 fizz fizz\n",
      "58 58 58\n",
      "59 59 59\n",
      "60 fizzbuzz fizzbuzz\n",
      "61 61 61\n",
      "62 62 62\n",
      "63 fizz fizz\n",
      "64 64 64\n",
      "65 buzz buzz\n",
      "66 fizz fizz\n",
      "67 67 67\n",
      "68 68 68\n",
      "69 fizz fizz\n",
      "70 buzz buzz\n",
      "71 71 71\n",
      "72 fizz fizz\n",
      "73 73 73\n",
      "74 74 74\n",
      "75 fizzbuzz fizzbuzz\n",
      "76 76 76\n",
      "77 77 77\n",
      "78 fizz fizz\n",
      "79 79 79\n",
      "80 80 buzz\n",
      "81 fizz fizz\n",
      "82 82 82\n",
      "83 83 83\n",
      "84 fizz fizz\n",
      "85 fizz buzz\n",
      "86 86 86\n",
      "87 fizz fizz\n",
      "88 88 88\n",
      "89 89 89\n",
      "90 fizzbuzz fizzbuzz\n",
      "91 91 91\n",
      "92 92 92\n",
      "93 fizz fizz\n",
      "94 94 94\n",
      "95 fizz buzz\n",
      "96 fizz fizz\n",
      "97 97 97\n",
      "98 98 98\n",
      "99 fizz fizz\n",
      "100 buzz buzz\n",
      "96 / 100\n"
     ]
    }
   ],
   "source": [
    "# finally we will solve this problem now:\n",
    "num_correct = 0\n",
    "\n",
    "for n in range(1, 101):\n",
    "    x = binary_encode(n)\n",
    "    predicted = argmax(feed_forward(network, x)[-1])\n",
    "    actual = argmax(fizz_buzz_encode(n))\n",
    "    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "    print(n, labels[predicted], labels[actual])\n",
    "\n",
    "    if predicted == actual:\n",
    "        num_correct += 1\n",
    "\n",
    "print(num_correct, \"/\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home work practice in pair\n",
    "1. Please explore and understand the decision tree theory and code<br>\n",
    "2. (6 points) Please work on Iris dataset using neural network you learned in this lecture, and try to make predictions for:<br>\n",
    "<br>['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "<br>[6.0, 3.0, 5.0, 0.6]\n",
    "<br>[6.0, 3.0, 5.0, 1.6]\n",
    "<br>[6.0, 3.0, 5.0, 2.6]\n",
    "3. (4 points) Please explore neural network on sklearn and train it on Iris dataset and make predictions on the three samples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.data)\n",
    "# print(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "def inputRecord(inputV):\n",
    "    resultVector = []\n",
    "    for i in range(len(inputV)):\n",
    "        if inputV[i] < 3:\n",
    "            resultVector+=[0,0,1]\n",
    "        elif inputV[i] < 6:\n",
    "            resultVector+=[0,1,0]\n",
    "        else:\n",
    "            resultVector+=[1,0,0]\n",
    "    return resultVector\n",
    "print(inputRecord([6.8,3.2,5.9,2.3]))\n",
    "\n",
    "def outputRecord(label):\n",
    "    if label == 0:\n",
    "        return [0,0,1]\n",
    "    elif label==1:\n",
    "        return [0,1,0]\n",
    "    else:\n",
    "        return [1,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "nn = []\n",
    "for i in data.data:\n",
    "    print(inputRecord(i))\n",
    "    nn.append(inputRecord(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[0, 1, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "nny = []\n",
    "for i in data.target:\n",
    "    print(outputRecord(i))\n",
    "    nny.append(outputRecord(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nn\n",
    "y = nny\n",
    "\n",
    "X_preprocessed = [inputRecord(x) for x in X]\n",
    "y_preprocessed = [outputRecord(label) for label in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y_preprocessed, test_size=0.2, random_state=42)\n",
    "NUM_HIDDEN = 10\n",
    "network = [\n",
    "    # Hidden layer: 12 inputs (4 features * 3 categories) -> NUM_HIDDEN outputs\n",
    "    [[random.random() for _ in range(12 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "\n",
    "    # Output layer: NUM_HIDDEN inputs -> 3 outputs\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(3)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # Add a constant.\n",
    "        print(\"Input vector length:\", len(input_with_bias))  # Print input vector length\n",
    "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "                  for neuron in layer]                    # for each neuron.\n",
    "        outputs.append(output)                            # Add to results.\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Neural Network:   0%|                                                                | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector length: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "vectors must be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Neural Network\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, y_train):\n\u001b[1;32m----> 7\u001b[0m         gradients \u001b[38;5;241m=\u001b[39m \u001b[43msqerror_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         network \u001b[38;5;241m=\u001b[39m [[gradient_step(neuron, grad, \u001b[38;5;241m-\u001b[39mlearning_rate) \u001b[38;5;28;01mfor\u001b[39;00m neuron, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(layer, layer_grad)] \u001b[38;5;28;01mfor\u001b[39;00m layer, layer_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(network, gradients)]\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36msqerror_gradients\u001b[1;34m(network, input_vector, target_vector)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mGiven a neural network, an input vector, and a target vector,\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mmake a prediction and compute the gradient of the squared error\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mloss with respect to the neuron weights.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m hidden_outputs, outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# gradients with respect to output neuron pre-activation outputs\u001b[39;00m\n\u001b[0;32m     13\u001b[0m output_deltas \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m output) \u001b[38;5;241m*\u001b[39m (output \u001b[38;5;241m-\u001b[39m target)\n\u001b[0;32m     14\u001b[0m                  \u001b[38;5;28;01mfor\u001b[39;00m output, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, target_vector)]\n",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m, in \u001b[0;36mfeed_forward\u001b[1;34m(neural_network, input_vector)\u001b[0m\n\u001b[0;32m     10\u001b[0m input_with_bias \u001b[38;5;241m=\u001b[39m input_vector           \u001b[38;5;66;03m# Add a constant.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput vector length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(input_with_bias))  \u001b[38;5;66;03m# Print input vector length\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m [\u001b[43mneuron_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneuron\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_with_bias\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute the output\u001b[39;00m\n\u001b[0;32m     13\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m layer]                    \u001b[38;5;66;03m# for each neuron.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(output)                            \u001b[38;5;66;03m# Add to results.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Then the input to the next layer is the output of this one\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mneuron_output\u001b[1;34m(weights, inputs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mneuron_output\u001b[39m(weights: Vector, inputs: Vector) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# weights includes the bias term, inputs includes a 1\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sigmoid(\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\linear_algebra.py:60\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(v, w)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(v: Vector, w: Vector) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(w), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectors must be same length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v_i \u001b[38;5;241m*\u001b[39m w_i \u001b[38;5;28;01mfor\u001b[39;00m v_i, w_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(v, w))\n",
      "\u001b[1;31mAssertionError\u001b[0m: vectors must be same length"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for _ in tqdm(range(epochs), desc=\"Training Neural Network\"):\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate) for neuron, grad in zip(layer, layer_grad)] for layer, layer_grad in zip(network, gradients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the provided input lists:\n",
      "Input: [6.0, 3.0, 5.0, 0.6]\n",
      "Prediction: [0.9930888735638442, 0.9744249878270596, 0.9981583219616275]\n",
      "Input: [6.0, 3.0, 5.0, 1.6]\n",
      "Prediction: [0.9930888735638442, 0.9744249878270596, 0.9981583219616275]\n",
      "Input: [6.0, 3.0, 5.0, 2.6]\n",
      "Prediction: [0.9930888735638442, 0.9744249878270596, 0.9981583219616275]\n"
     ]
    }
   ],
   "source": [
    "input_lists = [\n",
    "    [6.0, 3.0, 5.0, 0.6],\n",
    "    [6.0, 3.0, 5.0, 1.6],\n",
    "    [6.0, 3.0, 5.0, 2.6]\n",
    "]\n",
    "\n",
    "predictions = []\n",
    "for input_list in input_lists:\n",
    "    input_vector = inputRecord(input_list)\n",
    "    output = feed_forward(network, input_vector)[-1]\n",
    "    predictions.append(output)\n",
    "\n",
    "print(\"Predictions for the provided input lists:\")\n",
    "for input_list, prediction in zip(input_lists, predictions):\n",
    "    print(\"Input:\", input_list)\n",
    "    print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
