{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network example\n",
    "Let's explore implementation of Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import Vector, dot\n",
    "\n",
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)\n",
    "\n",
    "and_weights = [2., 2]\n",
    "and_bias = -3.\n",
    "\n",
    "assert perceptron_output(and_weights, and_bias, [1, 1]) == 1\n",
    "assert perceptron_output(and_weights, and_bias, [0, 1]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [1, 0]) == 0\n",
    "assert perceptron_output(and_weights, and_bias, [0, 0]) == 0\n",
    "\n",
    "or_weights = [2., 2]\n",
    "or_bias = -1.\n",
    "\n",
    "assert perceptron_output(or_weights, or_bias, [1, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 1]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [1, 0]) == 1\n",
    "assert perceptron_output(or_weights, or_bias, [0, 0]) == 0\n",
    "\n",
    "not_weights = [-2.]\n",
    "not_bias = 1.\n",
    "\n",
    "assert perceptron_output(not_weights, not_bias, [0]) == 1\n",
    "assert perceptron_output(not_weights, not_bias, [1]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Sigmoid function and feedforward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "    # weights includes the bias term, inputs includes a 1\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "    \n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # Add a constant.\n",
    "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "                  for neuron in layer]                    # for each neuron.\n",
    "        outputs.append(output)                            # Add to results.\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Build the XOR gate that we couldn’t build with a single perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and_gate = min\n",
    "# or_gate = max\n",
    "# xor_gate = lambda x, y: 0 if x == y else 1 \n",
    "\n",
    "xor_network = [# hidden layer\n",
    "               [[20., 20, -30],      # 'and' neuron\n",
    "                [20., 20, -10]],     # 'or'  neuron\n",
    "               # output layer\n",
    "               [[-60., 60, -30]]]    # '2nd input but not 1st input' neuron\n",
    "\n",
    "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
    "# final output, and the [0] gets the value out of the resulting vector\n",
    "assert 0.000 < feed_forward(xor_network, [0, 0])[-1][0] < 0.001\n",
    "assert 0.999 < feed_forward(xor_network, [1, 0])[-1][0] < 1.000\n",
    "assert 0.999 < feed_forward(xor_network, [0, 1])[-1][0] < 1.000\n",
    "assert 0.000 < feed_forward(xor_network, [1, 1])[-1][0] < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://learning.oreilly.com/library/view/data-science-from/9781492041122/assets/dsf2_1803.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://learning.oreilly.com/library/view/data-science-from/9781492041122/assets/dsf2_1803.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "we use data to train neural networks. The typical approach is an algorithm called backpropagation, which uses gradient descent or one of its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerror_gradients(network: List[List[Vector]],\n",
    "                      input_vector: Vector,\n",
    "                      target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll start by generating the training data and initializing our neural network with random weights\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# training data\n",
    "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "ys = [[0.], [1.], [1.], [0.]]\n",
    "\n",
    "# start with random weights\n",
    "network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "            [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "             [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "            # output layer: 2 inputs -> 1 output\n",
    "            [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neural net for xor: 100%|█████████████████████████████████████████████████████| 20000/20000 [00:01<00:00, 13955.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# As usual, we can train it using gradient descent\n",
    "from gradient_descent import gradient_step\n",
    "import tqdm\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "    for x, y in zip(xs, ys):\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "        # Take a gradient step for each neuron in each layer\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                    for neuron, grad in zip(layer, layer_grad)]\n",
    "                   for layer, layer_grad in zip(network, gradients)]\n",
    "\n",
    "# check that it learned XOR\n",
    "assert feed_forward(network, [0, 0])[-1][0] < 0.01\n",
    "assert feed_forward(network, [0, 1])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 0])[-1][0] > 0.99\n",
    "assert feed_forward(network, [1, 1])[-1][0] < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Fizz Buzz\n",
    "Print the numbers 1 to 100, except that if the number is divisible\n",
    "by 3, print \"fizz\"; if the number is divisible by 5, print \"buzz\";\n",
    "and if the number is divisible by 15, print \"fizzbuzz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(x: int) -> Vector:\n",
    "    if x % 15 == 0:\n",
    "        return [0, 0, 0, 1]\n",
    "    elif x % 5 == 0:\n",
    "        return [0, 0, 1, 0]\n",
    "    elif x % 3 == 0:\n",
    "        return [0, 1, 0, 0]\n",
    "    else:\n",
    "        return [1, 0, 0, 0]\n",
    "\n",
    "assert fizz_buzz_encode(2) == [1, 0, 0, 0]\n",
    "assert fizz_buzz_encode(6) == [0, 1, 0, 0]\n",
    "assert fizz_buzz_encode(10) == [0, 0, 1, 0]\n",
    "assert fizz_buzz_encode(30) == [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(x: int) -> Vector:\n",
    "    binary: List[float] = []\n",
    "\n",
    "    for i in range(10):\n",
    "        binary.append(x % 2)\n",
    "        x = x // 2\n",
    "\n",
    "    return binary\n",
    "\n",
    "#                             1  2  4  8 16 32 64 128 256 512\n",
    "assert binary_encode(0)   == [0, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(1)   == [1, 0, 0, 0, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(10)  == [0, 1, 0, 1, 0, 0, 0, 0,  0,  0]\n",
    "assert binary_encode(101) == [1, 0, 1, 0, 0, 1, 1, 0,  0,  0]\n",
    "assert binary_encode(999) == [1, 1, 1, 0, 0, 1, 1, 1,  1,  1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [binary_encode(n) for n in range(101, 1024)]\n",
    "ys = [fizz_buzz_encode(n) for n in range(101, 1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s create a neural network with random initial weights\n",
    "NUM_HIDDEN = 25\n",
    "\n",
    "network = [\n",
    "    # hidden layer: 10 inputs -> NUM_HIDDEN outputs\n",
    "    [[random.random() for _ in range(10 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "\n",
    "    # output_layer: NUM_HIDDEN inputs -> 4 outputs\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(4)]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fizz buzz (loss: 29.44): 100%|███████████████████████████████████████████████████████| 500/500 [01:53<00:00,  4.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from linear_algebra import squared_distance\n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "with tqdm.trange(500) as t:\n",
    "    for epoch in t:\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            predicted = feed_forward(network, x)[-1]\n",
    "            epoch_loss += squared_distance(predicted, y)\n",
    "            gradients = sqerror_gradients(network, x, y)\n",
    "\n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                        for neuron, grad in zip(layer, layer_grad)]\n",
    "                    for layer, layer_grad in zip(network, gradients)]\n",
    "\n",
    "        t.set_description(f\"fizz buzz (loss: {epoch_loss:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our network will produce a four-dimensional vector of numbers, but we want a single prediction. \n",
    "#We’ll do that by taking the argmax, which is the index of the largest value:\n",
    "def argmax(xs: list) -> int:\n",
    "    \"\"\"Returns the index of the largest value\"\"\"\n",
    "    return max(range(len(xs)), key=lambda i: xs[i])\n",
    "\n",
    "assert argmax([0, -1]) == 0               # items[0] is largest\n",
    "assert argmax([-1, 0]) == 1               # items[1] is largest\n",
    "assert argmax([-1, 10, 5, 20, -3]) == 3   # items[3] is largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23615812252723245, 0.6544816976593077, 0.7428466359173387, 0.8871393899936986, 0.6834174826949725, 0.847209593846943, 0.784479914051605, 0.16071613819910335, 0.043692765840927295, 0.7387786394375336, 0.525918898422267, 0.9978655049817808, 0.1648905413828592]\n",
      "[0.3852699376295755, 0.28778357569091084, 0.8786973176916403, 0.4836957619564889, 0.9136495815761022, 0.7071900029051184, 0.9988061683109193, 0.5997790159723878, 0.9761591529166639, 0.17340633728780475, 0.44168006118571757, 0.5783912042361588, 0.978295910581493]\n",
      "[0.5678798241127215, 0.8652649809797757, 0.6285055588490285, 0.5124010252172735, 0.39144138423523445, 0.36863404796259336, 0.29521803737147234, 0.2113699708500314, 0.9625770244136284, 0.5364612161156107, 0.8658695364577362, 0.8849637610619134, 0.9422998611296344]\n",
      "[0.238166806096774, 0.33772889244543136, 0.6331398796026679, 0.32205404522698333, 0.143928094536648, 0.7598592136554477, 0.5503918988479339, 0.5365230195570996, 0.7104710384938592, 0.11474025170264657, 0.9219090077794494, 0.4798230842114579, 0.6918288929873827]\n",
      "[0.6003242608751183, 0.6052329260786896, 0.7099116802375812, 0.08879564360393677, 0.4967332177217382, 0.21028942088591251, 0.3896838521468041, 0.5117457896622603, 0.353948636681213, 0.4067129069218388, 0.7308698223586158, 0.04332726591892744, 0.9565935894623465]\n",
      "[0.6039175229614193, 0.1635546374835204, 0.5572187636668521, 0.08093556203584729, 0.5014737046343523, 0.6886314264419748, 0.4197779358470568, 0.3141813192404608, 0.6737141068483354, 0.9352851860227156, 0.8736358411391872, 0.3853682134632209, 0.8634022393426443]\n",
      "[0.11503416219764384, 0.05872059862795831, 0.9831869688608714, 0.7629087214628383, 0.6149884292498408, 0.5587807553866757, 0.308878947406684, 0.8990821645771516, 0.8527504497478022, 0.48171500395965516, 0.22038280178642822, 0.6763951785092713, 0.7261892414106231]\n",
      "[0.9955062640731493, 0.7903781654547807, 0.09124686201014309, 0.9899408446068302, 0.8545128156252924, 0.5825239383657913, 0.33093114909305965, 0.7329277010015032, 0.596621304710382, 0.09576558248049771, 0.5635126250604243, 0.020196929881345582, 0.7886966474017147]\n",
      "[0.8235857556247053, 0.7301155053979844, 0.09147841346093333, 0.5880937194102299, 0.391395566685379, 0.12839305231099996, 0.8920904440862538, 0.9422941202449211, 0.9222836912506559, 0.531217828083057, 0.8644284895151452, 0.19783621488305458, 0.295483940532401]\n",
      "[0.9087112410642484, 0.5903739702044343, 0.22601580721813908, 0.13009744790142153, 0.22831192059724947, 0.4960068821048771, 0.30348909754506237, 0.7344217420832106, 0.2713110089632601, 0.07843760502600283, 0.898212647761514, 0.6638393059891143, 0.9740642901678284]\n",
      "[0.18199708460923214, 0.8703875585555161, 0.0171955477554252, 0.537822132248361, 0.4797424352027706, 0.12619639005780303, 0.8151187132395588, 0.27085193307372346, 0.8984341526048886, 0.6994134457256543, 0.8523605517391581, 0.866480963522708, 0.7917829796606796]\n",
      "[0.7364883513791404, 0.0040705196130683685, 0.14343000817233487, 0.20713203951965786, 0.5774824148077424, 0.0033746169373203294, 0.12715652693416224, 0.48496670677178333, 0.04106427359074971, 0.3187463875828569, 0.2199883436397435, 0.17440705341566887, 0.3166056473863371]\n",
      "[0.8812167677476193, 0.23144265965090172, 0.6491588252584514, 0.7339981814871853, 0.6752805837056971, 0.1887454621522362, 0.349800724462744, 0.27215703839529537, 0.5386463298546706, 0.9690351581776423, 0.21787257705434104, 0.5523706927334204, 0.0654766349136886]\n",
      "[0.37556716884520036, 0.9541729509085174, 0.9084773112215223, 0.09520613787254661, 0.8525258760040951, 0.7157063921559241, 0.9179234572892788, 0.46081546056405065, 0.42255500898955534, 0.896152971189902, 0.5361573114624753, 0.7613674982733807, 0.17764210506576916]\n",
      "[0.06843333888844061, 0.44049382935637826, 0.32705153060084224, 0.5118734542374528, 0.3443233358347426, 0.8625880136342319, 0.7356049397731748, 0.38395686112029637, 0.12586647102406257, 0.7098445721898051, 0.5410855529400107, 0.1519452829365675, 0.034784751425624205]\n",
      "[0.6167001831805176, 0.5162479487166513, 0.575455278073261, 0.41586121328275316, 0.4687118396419784, 0.3914262130097276, 0.0877253494975695, 0.5353051230468925, 0.1216042303049456, 0.673488620419538, 0.7494181741285856, 0.1679290386459351, 0.20151169771708288]\n",
      "[0.2407111234600643, 0.5985101191549148, 0.40649258397268306, 0.887530303214221, 0.5479671672926812, 0.5256362721227988, 0.218493889956074, 0.09085725390512467, 0.9247099895721538, 0.09963898743095712, 0.13022502216543697, 0.19530836157677256, 0.5768263676747284]\n",
      "[0.6390334414021484, 0.43163650185433855, 0.394897586589502, 0.6410272860001994, 0.26209445053826896, 0.8004504477332421, 0.6400497304670643, 0.6028626247397026, 0.028876605263293942, 0.34537429347845405, 0.7688597946495157, 0.20590764062147937, 0.6444941761692219]\n",
      "[0.974719719570854, 0.44054047923715856, 0.5179472616471856, 0.2120401324754475, 0.007016764159117672, 0.23660113255486837, 0.47183996507084847, 0.6042713692491128, 0.8359294284057364, 0.29001885199683053, 0.3290444443971976, 0.7206486384349531, 0.6642812631821072]\n",
      "[0.7145224980820698, 0.8772847627912127, 0.08868130754144987, 0.12448637803366214, 0.49634252764708475, 0.6126045928996905, 0.6540715179469995, 0.23009749398538937, 0.13609598136959744, 0.9214358618820164, 0.24007085604124967, 0.01782823715299553, 0.2829125385282707]\n",
      "[0.517199924920388, 0.6333540258568372, 0.7392090236929509, 0.1455887639206288, 0.5078761601557434, 0.3201941451689181, 0.7246835162906685, 0.359494230923597, 0.8111433672967839, 0.1916196286695403, 0.9947020060955988, 0.5214396320083504, 0.4238451320754397]\n",
      "[0.7256572077992826, 0.3788453295436597, 0.035478809049156346, 0.4408764248870518, 0.287794132750106, 0.6612116407980199, 0.5267145722212628, 0.8300981193956986, 0.4892791237676378, 0.15534589130733623, 0.14860433247818117, 0.5726231592103207, 0.264873986997173]\n",
      "[0.2115992837903965, 0.9417827141119313, 0.13929625929477885, 0.9160704899499437, 0.5362275348888094, 0.9369081942697894, 0.839033082413032, 0.29887829534749877, 0.47019664625002433, 0.08528300138159317, 0.36667135095139414, 0.9267272807547686, 0.10099967937376109]\n",
      "[0.24563724190313296, 0.04272642009040806, 0.8608282633918696, 0.6835875869098313, 0.5893452173938375, 0.46565282956871523, 0.26004358185176, 0.5858435351912367, 0.7027155798859228, 0.7929996503619103, 0.16247418699852767, 0.6253035822657671, 0.6788257943277809]\n",
      "[0.5811737548420965, 0.7277542112741848, 0.5177836355556434, 0.9546400643636819, 0.6501853684593555, 0.6280476767062497, 0.013157764274433492, 0.14354178836173903, 0.6010715341181541, 0.7669116547094946, 0.1442581181538788, 0.6368285787960748, 0.15434637552672625]\n",
      "[0.7632040441829882, 0.8212052255275862, 0.6206139860847025, 0.06791389812235493, 0.2794384367425734, 0.26953129189431313, 0.4678335381835361, 0.7800651360897125, 0.578317831604538, 0.9919561150782095, 0.7082471480573831, 0.1413590671681363, 0.9791291464232612]\n",
      "[0.05861278136847703, 0.3328572009052253, 0.6372512758358849, 0.3904865526077951, 0.022172495602277742, 0.29648458699067093, 0.24186668703182468, 0.7761490351643135, 0.5925539338651099, 0.14399589782846844, 0.8726057225681068, 0.21299855748548913, 0.3196783510899732]\n",
      "[0.8747233972749223, 0.7652163598722272, 0.420831088559633, 0.5192717429080365, 0.9791690195290002, 0.7108063573530919, 0.7157291514387905, 0.6557821745669479, 0.9882384716588185, 0.9240089297326894, 0.2982393659274032, 0.44515795744244846, 0.6356861164396894]\n",
      "[0.2369004175129127, 0.647283950908657, 0.9032728580755631, 0.30627568874273314, 0.3679225381088106, 0.44993963857191543, 0.38631768499341557, 0.643322418982538, 0.051969212528645614, 0.7767360789424755, 0.2853422873496436, 0.6220060867962606, 0.42386829456841024]\n",
      "[0.6112890902381699, 0.568948699113673, 0.5175578459699145, 0.16032315669548503, 0.007522748908312948, 0.10697676122016819, 0.3848933928307545, 0.25709748415529443, 0.4847970223442596, 0.470831366365695, 0.5153883353185527, 0.13270970142391947, 0.49740877947309825]\n",
      "[0.9504868006222581, 0.17198736737656162, 0.015543365536247244, 0.33888836399997757, 0.7084993698221543, 0.8607508302791741, 0.10926794750653401, 0.03120275911746173, 0.3101367452934325, 0.6220344731254223, 0.9204068496267381, 0.33042353697347804, 0.7793622980362773]\n",
      "[0.12751669408471467, 0.6409725556070028, 0.2499635171703265, 0.7611782753543603, 0.9121258666543576, 0.4414066238236891, 0.6872183519440834, 0.35403760841256093, 0.849072556468551, 0.4101337445127272, 0.5840778656718704, 0.9864544725999015, 0.5576371908247157]\n",
      "[0.4528302485571678, 0.09606024634505095, 0.949620804218125, 0.5248997126425485, 0.7007913171221052, 0.6545821836591169, 0.23744354690094804, 0.6370706760944617, 0.0966287957270211, 0.057288405860308034, 0.8409133203054258, 0.6007820455016075, 0.3013725748550947]\n",
      "[0.526839592489771, 0.557902482664483, 0.6777782688839528, 0.00013428158142114732, 0.14444836975277553, 0.09316983326750472, 0.7530860479007456, 0.4523347157752784, 0.19839660259158298, 0.37448605806775714, 0.6694545937441199, 0.4612240736300318, 0.5458590475559781]\n",
      "[0.937533270193081, 0.40032319735627, 0.10325392339181139, 0.10713844991944721, 0.7247173578122594, 0.31246957588771773, 0.11503805593416416, 0.7781811602505925, 0.8888195685485272, 0.10235684025852576, 0.6165138355544916, 0.7404883582959025, 0.2458346480209498]\n",
      "[0.8366688796226954, 0.6840124142509009, 0.4445697975573991, 0.1657156233880004, 0.2578327872174532, 0.8294681585898959, 0.16780782818687778, 0.7046333464365101, 0.5709425809029731, 0.5607179953047123, 0.016331672376246664, 0.12270128801796121, 0.30980456682193547]\n",
      "[0.6303292421141978, 0.3814354181934909, 0.256491852725225, 0.3838408233935168, 0.4631157565499274, 0.5945831872871431, 0.5597474562893839, 0.3680180176092136, 0.42546227305976847, 0.8062392473844322, 0.5890153230795825, 0.9704846959334615, 0.6024737151988286]\n",
      "\n",
      "\n",
      "[0.23615812252723245, 0.6544816976593077, 0.7428466359173387, 0.8871393899936986, 0.6834174826949725, 0.847209593846943, 0.784479914051605, 0.16071613819910335, 0.043692765840927295, 0.7387786394375336, 0.525918898422267, 0.9978655049817808, 0.1648905413828592]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "vectors must be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[0;32m      5\u001b[0m     x \u001b[38;5;241m=\u001b[39m binary_encode(n)\n\u001b[1;32m----> 6\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m argmax(\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      7\u001b[0m     actual \u001b[38;5;241m=\u001b[39m argmax(fizz_buzz_encode(n))\n\u001b[0;32m      8\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(n), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfizz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuzz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfizzbuzz\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[41], line 27\u001b[0m, in \u001b[0;36mfeed_forward\u001b[1;34m(neural_network, input_vector)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m input_with_bias \u001b[38;5;241m=\u001b[39m input_vector \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m1\u001b[39m]              \u001b[38;5;66;03m# Add a constant.\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m output \u001b[38;5;241m=\u001b[39m [\u001b[43mneuron_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneuron\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_with_bias\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute the output\u001b[39;00m\n\u001b[0;32m     28\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m neuron \u001b[38;5;129;01min\u001b[39;00m layer]                    \u001b[38;5;66;03m# for each neuron.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(output)                            \u001b[38;5;66;03m# Add to results.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Then the input to the next layer is the output of this one\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[41], line 10\u001b[0m, in \u001b[0;36mneuron_output\u001b[1;34m(weights, inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sigmoid(\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\linear_algebra.py:60\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(v, w)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(v: Vector, w: Vector) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(w), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectors must be same length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v_i \u001b[38;5;241m*\u001b[39m w_i \u001b[38;5;28;01mfor\u001b[39;00m v_i, w_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(v, w))\n",
      "\u001b[1;31mAssertionError\u001b[0m: vectors must be same length"
     ]
    }
   ],
   "source": [
    "# finally we will solve this problem now:\n",
    "num_correct = 0\n",
    "\n",
    "for n in range(1, 101):\n",
    "    x = binary_encode(n)\n",
    "    predicted = argmax(feed_forward(network, x)[-1])\n",
    "    actual = argmax(fizz_buzz_encode(n))\n",
    "    labels = [str(n), \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "    print(n, labels[predicted], labels[actual])\n",
    "\n",
    "    if predicted == actual:\n",
    "        num_correct += 1\n",
    "\n",
    "#print(num_correct, \"/\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home work practice in pair\n",
    "1. Please explore and understand the decision tree theory and code<br>\n",
    "2. (6 points) Please work on Iris dataset using neural network you learned in this lecture, and try to make predictions for:<br>\n",
    "<br>['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "<br>[6.0, 3.0, 5.0, 0.6]\n",
    "<br>[6.0, 3.0, 5.0, 1.6]\n",
    "<br>[6.0, 3.0, 5.0, 2.6]\n",
    "3. (4 points) Please explore neural network on sklearn and train it on Iris dataset and make predictions on the three samples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.data)\n",
    "# print(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "def inputRecord(inputV):\n",
    "    resultVector = []\n",
    "    for i in range(len(inputV)):\n",
    "        if inputV[i] < 3:\n",
    "            resultVector+=[0,0,1]\n",
    "        elif inputV[i] < 6:\n",
    "            resultVector+=[0,1,0]\n",
    "        else:\n",
    "            resultVector+=[1,0,0]\n",
    "    return resultVector\n",
    "print(inputRecord([6.8,3.2,5.9,2.3]))\n",
    "\n",
    "def outputRecord(label):\n",
    "    if label == 0:\n",
    "        return [0,0,1]\n",
    "    elif label==1:\n",
    "        return [0,1,0]\n",
    "    else:\n",
    "        return [1,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "nn = []\n",
    "for i in data.data:\n",
    "    # print(inputRecord(i))\n",
    "    nn.append(inputRecord(i))\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nny = []\n",
    "for i in data.target:\n",
    "    #print(outputRecord(i))\n",
    "    nny.append(outputRecord(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "X = nn\n",
    "y = nny\n",
    "\n",
    "X_preprocessed = X\n",
    "y_preprocessed = y\n",
    "print(X_preprocessed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y_preprocessed, test_size=0.2, random_state=42)\n",
    "NUM_HIDDEN = 13\n",
    "network = [\n",
    "    # Hidden layer: 12 inputs (4 features * 3 categories) -> NUM_HIDDEN outputs\n",
    "    [[random.random() for _ in range(12 + 1)] for _ in range(NUM_HIDDEN)],\n",
    "\n",
    "    # Output layer: NUM_HIDDEN inputs -> 3 outputs\n",
    "    [[random.random() for _ in range(NUM_HIDDEN + 1)] for _ in range(3)]\n",
    "]\n",
    "print(len(network[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # Add a constant.\n",
    "        print(\"Input vector length:\", len(input_with_bias))  # Print input vector length\n",
    "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "                  for neuron in layer]                    # for each neuron.\n",
    "        outputs.append(output)                            # Add to results.\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Neural Network: 100%|███████████████████████████████████████████████████████| 100/100 [00:01<00:00, 89.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for _ in tqdm(range(epochs), desc=\"Training Neural Network\"):\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        gradients = sqerror_gradients(network, x, y)\n",
    "        network = [[gradient_step(neuron, grad, -learning_rate) for neuron, grad in zip(layer, layer_grad)] for layer, layer_grad in zip(network, gradients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the provided input lists:\n",
      "Input: [6.0, 3.0, 5.0, 0.6]\n",
      "Prediction: [0.659326294127269, 0.355811692958883, 0.0025162869212524705]\n",
      "Input: [6.0, 3.0, 5.0, 1.6]\n",
      "Prediction: [0.659326294127269, 0.355811692958883, 0.0025162869212524705]\n",
      "Input: [3.0, 3.0, 5.0, 2.6]\n",
      "Prediction: [0.16025740513433614, 0.8368539776889957, 0.01395730982170832]\n"
     ]
    }
   ],
   "source": [
    "input_lists = [\n",
    "    [6.0, 3.0, 5.0, 0.6],\n",
    "    [6.0, 3.0, 5.0, 1.6],\n",
    "    [3.0, 3.0, 5.0, 2.6]\n",
    "]\n",
    "\n",
    "predictions = []\n",
    "for input_list in input_lists:\n",
    "    input_vector = inputRecord(input_list)\n",
    "    output = feed_forward(network, input_vector)[-1]\n",
    "    predictions.append(output)\n",
    "\n",
    "print(\"Predictions for the provided input lists:\")\n",
    "for input_list, prediction in zip(input_lists, predictions):\n",
    "    print(\"Input:\", input_list)\n",
    "    print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
